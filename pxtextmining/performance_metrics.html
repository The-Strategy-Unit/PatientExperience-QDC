<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.335">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="YiWen Hon">
<meta name="dcterms.date" content="2023-03-01">

<title>PatientExperience-QDC website - Performance metrics</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../docs/_assets/favicon.ico" rel="icon">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>


<link rel="stylesheet" href="../_assets/style/styles.css">
<meta property="og:title" content="PatientExperience-QDC website - Performance metrics">
<meta property="og:description" content="How do we know if the model works?">
<meta property="og:site-name" content="PatientExperience-QDC website">
</head>

<body class="nav-sidebar floating nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link active" href="../index.html" aria-current="page"><i class="bi bi-book" role="img">
</i> 
 <span class="menu-text">About PatientExperience-QDC</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../updates.html"><i class="bi bi-rss" role="img">
</i> 
 <span class="menu-text">Updates</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-help" role="button" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Help</span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-help">    
        <li>
    <a class="dropdown-item" href="https://github.com/CDU-data-science-team/PatientExperience-QDC/issues"><i class="bi bi-bug" role="img">
</i> 
 <span class="dropdown-text">Report a Bug</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://github.com/CDU-data-science-team/PatientExperience-QDC/discussions"><i class="bi bi-chat-right-text" role="img">
</i> 
 <span class="dropdown-text">Ask a Question</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/CDU-data-science-team/PatientExperience-QDC"><i class="bi bi-github" role="img" aria-label="GitHub">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title">Performance metrics</h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">Homepage</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../background.html" class="sidebar-item-text sidebar-link">Background and roadmap</a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../framework/index.html" class="sidebar-item-text sidebar-link">Qualitative Framework</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../framework/framework2.html" class="sidebar-item-text sidebar-link">framework2</a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../pxtextmining/index.html" class="sidebar-item-text sidebar-link">Machine learning (pxtextmining)</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pxtextmining/performance_metrics.html" class="sidebar-item-text sidebar-link active">Performance metrics</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pxtextmining/model_selection.html" class="sidebar-item-text sidebar-link">Model selection</a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../dashboard/index.html" class="sidebar-item-text sidebar-link">Experiences dashboard</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../dashboard/dashboard2.html" class="sidebar-item-text sidebar-link">Additional Analysis for ExperiencesDashboard</a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../acknowledgements.html" class="sidebar-item-text sidebar-link">Acknowledgements</a>
  </div>
</li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#common-terminology-in-classification-models" id="toc-common-terminology-in-classification-models" class="nav-link active" data-scroll-target="#common-terminology-in-classification-models">Common terminology in classification models</a></li>
  <li><a href="#single-label-performance-metrics" id="toc-single-label-performance-metrics" class="nav-link" data-scroll-target="#single-label-performance-metrics">Single label performance metrics</a>
  <ul>
  <li><a href="#accuracy" id="toc-accuracy" class="nav-link" data-scroll-target="#accuracy">Accuracy</a></li>
  <li><a href="#recall" id="toc-recall" class="nav-link" data-scroll-target="#recall">Recall</a></li>
  <li><a href="#precision" id="toc-precision" class="nav-link" data-scroll-target="#precision">Precision</a></li>
  <li><a href="#f1-score" id="toc-f1-score" class="nav-link" data-scroll-target="#f1-score">F1 score</a></li>
  </ul></li>
  <li><a href="#multilabel-performance-metrics" id="toc-multilabel-performance-metrics" class="nav-link" data-scroll-target="#multilabel-performance-metrics">Multilabel performance metrics</a>
  <ul>
  <li><a href="#hamming-loss" id="toc-hamming-loss" class="nav-link" data-scroll-target="#hamming-loss">Hamming loss</a></li>
  <li><a href="#jaccard-score" id="toc-jaccard-score" class="nav-link" data-scroll-target="#jaccard-score">Jaccard score</a></li>
  <li><a href="#averaging-single-label-metrics" id="toc-averaging-single-label-metrics" class="nav-link" data-scroll-target="#averaging-single-label-metrics">Averaging single label metrics</a></li>
  </ul></li>
  <li><a href="#conclusion-which-metrics-to-use-for-pxtextmining-phase-2" id="toc-conclusion-which-metrics-to-use-for-pxtextmining-phase-2" class="nav-link" data-scroll-target="#conclusion-which-metrics-to-use-for-pxtextmining-phase-2">Conclusion: which metrics to use for pxtextmining Phase 2?</a></li>
  <li><a href="#a-note-on-baselinedummy-models" id="toc-a-note-on-baselinedummy-models" class="nav-link" data-scroll-target="#a-note-on-baselinedummy-models">A note on baseline/dummy models</a></li>
  <li><a href="#bibliography" id="toc-bibliography" class="nav-link" data-scroll-target="#bibliography">Bibliography</a></li>
  </ul>
<div class="toc-actions"><div><i class="bi bi-github"></i></div><div class="action-links"><p><a href="https://github.com/CDU-data-science-team/PatientExperience-QDC/issues/new" class="toc-action">Report an issue</a></p></div></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block">Performance metrics</h1>
<p class="subtitle lead">How do we know if the model works?</p>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>YiWen Hon </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">March 1, 2023</p>
    </div>
  </div>
  
    
  </div>
  

</header>

<p>When we code machine learning models, we need to be able to measure how well they are performing. Performance metrics are the ‘scoring systems’ that we use to measure a model’s predictions. We keep a subset of the data aside that the model has never seen when training, and compare the model’s predicted labels with the real labels for that data. This is called the ‘test set’.</p>
<p>We would choose different performance metrics depending on our aims and objectives for the model. For this project, we are classifying patient comments, trying to label them by topic. Each comment can be referred to as a ‘sample’. A single-label classification model would assign each sample, or patient comment, to only one corresponding topic. In single-label classification, the most commonly used performance metrics are: accuracy, recall, precision, and f1 score.</p>
<section id="common-terminology-in-classification-models" class="level2">
<h2 class="anchored" data-anchor-id="common-terminology-in-classification-models">Common terminology in classification models</h2>
<p>The key terminology to understand is the concept of true positives, true negatives, false positives, and false negatives.</p>
<p><strong>True positive</strong> is when the sample has the label, and the model correctly predicts this label. An example is given in <a href="#tbl-TPTN">Table&nbsp;1</a>.</p>
<div id="tbl-TPTN" class="anchored">
<table class="table">
<caption>Table&nbsp;1: True positive and True negative example</caption>
<colgroup>
<col style="width: 24%">
<col style="width: 39%">
<col style="width: 36%">
</colgroup>
<thead>
<tr class="header">
<th><strong>Comment</strong></th>
<th><strong>Labels given by qualitative researcher</strong></th>
<th><strong>Labels given by hypothetical model</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>“I really enjoyed dinner”</td>
<td>Food &amp; diet</td>
<td>Food &amp; diet</td>
</tr>
</tbody>
</table>
</div>
<p>In this example, the model has also correctly predicted that the comment is NOT any other label, such as <code>Medication</code>. The comment in question has not been tagged as <code>Medication</code>, and the model was able to correctly identify this, predicting only <code>Food &amp; diet</code> as the label instead. Because it predicted a negative for the label <code>Medication</code> for the comment ‘I really enjoyed dinner’, and this is the same as the real labelling for the comment, this is called a <strong>true negative</strong> for that label.</p>
<p>In contrast with true positives and true negatives, false positive and false negative are when the model has made a mistake. For the example in <a href="#tbl-FPFN">Table&nbsp;2</a>, the model has labelled the comment with <code>Medication</code>, although this is not a real label for the data. This means that it has made a <strong>false positive</strong> prediction for the <code>Medication</code> label.</p>
<p>It has also missed that the real label is <code>Food &amp; diet</code>, meaning that it has made a <strong>false negative</strong> prediction for the label <code>Food &amp; diet</code>.</p>
<div id="tbl-FPFN" class="anchored">
<table class="table">
<caption>Table&nbsp;2: False positive and False negative example</caption>
<colgroup>
<col style="width: 24%">
<col style="width: 39%">
<col style="width: 36%">
</colgroup>
<thead>
<tr class="header">
<th><strong>Comment</strong></th>
<th><strong>Labels given by qualitative researcher</strong></th>
<th><strong>Labels given by hypothetical model</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>“I really enjoyed dinner”</td>
<td>Food &amp; diet</td>
<td>Medication</td>
</tr>
</tbody>
</table>
</div>
<p>The total number of true positives, true negatives, false positives and false negatives are usually put together in a table called a confusion matrix.</p>
</section>
<section id="single-label-performance-metrics" class="level2">
<h2 class="anchored" data-anchor-id="single-label-performance-metrics">Single label performance metrics</h2>
<section id="accuracy" class="level3">
<h3 class="anchored" data-anchor-id="accuracy">Accuracy</h3>
<p>Accuracy is usually the concept that is most simple to grasp. It is the total number of correct predictions divided by the sum of the overall number of predictions. Range is between 0 to 1, the closer to 1 the better.</p>
<p>However, it’s not always the best metric to use, particularly with imbalanced datasets. In <a href="#tbl-accuracy">Table&nbsp;3</a> below, the model mostly predicts negatives. Of the 10 real positive values it is only able to predict this correctly 1 time. This means that the accuracy score is high (0.91), but it’s not very good at capturing the positive class when it does occur.</p>
<div id="tbl-accuracy" class="anchored">
<table class="table">
<caption>Table&nbsp;3: High accuracy is not always good!</caption>
<thead>
<tr class="header">
<th></th>
<th>Predicted Negative</th>
<th>Predicted Positive</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Actual Negative</td>
<td>True Negatives: 90</td>
<td>False Positives: 0</td>
</tr>
<tr class="even">
<td>Actual Positive</td>
<td>False Negatives: 9</td>
<td>True Positives: 1</td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="recall" class="level3">
<h3 class="anchored" data-anchor-id="recall">Recall</h3>
<p>Recall measures the ability of the model to detect occurrences of a class. For our example above, the model which had an accuracy of 0.91 only has a recall of 0.1. Recall is best used when it is important to identify as many occurrences of a class as possible, reducing false negatives but potentially increasing false positives. Range is between 0 to 1, the higher the better. The mathematical formula for calculating recall is:</p>
<p>True Positives / (True Positives + False Negatives)</p>
<p>I like to use the analogy of ‘recall’ as trying to optimise fishing by using a net. You are happy to get false positives (e.g.&nbsp;stones, debris) in your catch so that you can capture more fish overall.</p>
<p>Scenarios where a model might be optimised for recall include where you are trying to predict fraudulent bank transactions. It might be safer to flag more transactions as fraudulent (increasing potential false positives) and check them, in order to avoid false negatives.</p>
<p>In the context of pxtextmining, a model that is optimised for recall would perhaps assign more ‘false positives’. So you would be likely to get lots of labels which might not actually be applicable for a particular text. For a comment like ‘I really enjoyed dinner’, you might get the real label <code>Food &amp; diet</code>, but also incorrect false positive labels such as <code>Medication</code> and <code>Communication</code> as well. The model will be more indiscriminate when assigning labels, so as not to accidentally miss a label.</p>
</section>
<section id="precision" class="level3">
<h3 class="anchored" data-anchor-id="precision">Precision</h3>
<p>Precision measures the ability of a model to avoid false alarms for a class, or the confidence of a model when predicting a specific class. It’s best used when it is important to be correct when identifying a class, reducing false positives but potentially increasing false negatives. Range is between 0 to 1, the higher the better. For the example in <a href="#tbl-accuracy">Table&nbsp;3</a> the precision would be 1:</p>
<p>True Positives / (True Positives + False Positives) <br>= 1 / (1 + 0) <br>= 1</p>
<p>The model did not identify any false positives at all, although it did have 9 false negatives, missing 9/10 of the target of interest. As we know, the recall was poor (0.1).</p>
<p>The model with high precision but low recall is very good at avoiding false positives, but unfortunately it also made a lot of false negative predictions. The analogy I like to use for understanding optimising for precision is that it is like trying to fish with a spear. You want to be sure that when you catch something, it is definitely a fish. You’re trying to minimise false positives, or anything that is not a fish, but you may miss some fish in doing so (maybe increasing false negatives).</p>
<p>Scenarios where a model might be optimised for precision include where you are trying to predict the safety of seatbelts. It is important to avoid false positives in this scenario. The cost of throwing away a potentially good seatbelt is relatively low, compared to the cost of equipping a car with a faulty seatbelt.</p>
<p>In the context of pxtextmining, a model that is optimised for precision may try to be more certain about a label before predicting it for a comment. So for a comment like ‘I really enjoyed dinner and the ward was comfortable’, which has the real labels <code>Food &amp; diet</code> and <code>Environment</code>, the model may only be very certain that it is about <code>Food &amp; diet</code> and label it as such. This means that it might miss the <code>Environment</code> label. This model would therefore produce more false negatives.</p>
<p>Precision and recall cannot both be increased at the same time, there is a tradeoff to be made between the two.</p>
<p>Still confused? <a href="https://www.youtube.com/watch?v=qWfzIYCvBqo">This video may help</a>.</p>
</section>
<section id="f1-score" class="level3">
<h3 class="anchored" data-anchor-id="f1-score">F1 score</h3>
<p>F1 score is sometimes known as the harmonic mean of recall and precision. It’s an attempt to generalise the two. The range for this is between 0 to 1, the higher the better.</p>
<p>In phase 1 of the pxtextmining project, the metric that was optimised was class balance accuracy (not to be confused with balanced accuracy score).[1] This was a type of averaged accuracy score obtained across the different classes.</p>
</section>
</section>
<section id="multilabel-performance-metrics" class="level2">
<h2 class="anchored" data-anchor-id="multilabel-performance-metrics">Multilabel performance metrics</h2>
<p>In a multilabel model, one sample can have more than one label. The exact number of labels assigned to the sample can vary. For example, we could label films like this:</p>
<table class="table">
<thead>
<tr class="header">
<th><strong>Film</strong></th>
<th><strong>Labels</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>The Mummy</td>
<td><code>Action</code> <code>Adventure</code> <code>Comedy</code></td>
</tr>
<tr class="even">
<td>Shrek</td>
<td><code>Comedy</code> <code>Animation</code></td>
</tr>
</tbody>
</table>
<p>Performance measurement for models that perform multilabel classification is tricky. There is little consensus in the literature about which metric to select. This is because, as with the single-label example, it largely depends on what we’re trying to accomplish. The performance of the multi-label learning algorithm should therefore be tested on a broad range of metrics instead of only on the one being optimized.[2], [3] The current popular methods for reporting the performance of a multi-label classifier include the Hamming loss, precision, recall, and F-score for each class with the micro, macro, and weighted average of these scores from all classes.[4]</p>
<div id="tbl-multilabel" class="anchored">
<table class="table">
<caption>Table&nbsp;4: Example of multilabel scenario</caption>
<thead>
<tr class="header">
<th><strong>Film</strong></th>
<th><strong>Real Labels</strong></th>
<th><strong>Predicted Labels</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>The Mummy</td>
<td><code>Action</code> <code>Adventure</code> <code>Comedy</code></td>
<td><code>Action</code> <code>Adventure</code></td>
</tr>
<tr class="even">
<td>Shrek</td>
<td><code>Comedy</code> <code>Animation</code></td>
<td><code>Comedy</code> <code>Animation</code></td>
</tr>
</tbody>
</table>
</div>
<section id="hamming-loss" class="level3">
<h3 class="anchored" data-anchor-id="hamming-loss">Hamming loss</h3>
<p>The Hamming loss is the fraction of labels that are incorrectly predicted. This ranges between 0 and 1, the lower the better. In the example in <a href="#tbl-multilabel">Table&nbsp;4</a> above, there would be a Hamming loss of 0.125 as it only didn’t manage to capture 1 of the labels.</p>
</section>
<section id="jaccard-score" class="level3">
<h3 class="anchored" data-anchor-id="jaccard-score">Jaccard score</h3>
<p>Jaccard similarity index is the size of the intersection of the predicted labels and the true labels divided by the size of the union of the predicted and true labels. It ranges from 0 to 1, and 1 is the perfect score. However, if there are no true or predicted labels, the sklearn implementation of Jaccard will return an error.</p>
</section>
<section id="averaging-single-label-metrics" class="level3">
<h3 class="anchored" data-anchor-id="averaging-single-label-metrics">Averaging single label metrics</h3>
<p>All of the usual classification metrics (Recall, Precision, F1 score) can be used in multilabel. For example, you could get a recall, precision, and F1 score for every label, on a one-vs-rest approach.</p>
<p>In multilabel, accuracy is sometimes also called ‘exact match accuracy’ or ‘subset accuracy’. This is quite a harsh metric, as all of the labels for the sample must be correct. In the example in <a href="#tbl-multilabel">Table&nbsp;4</a>, there would be a subset accuracy of only 0.5 as the model did not accurately predict all of the correct labels for the first sample, although it got 5 out of 6 labels right.</p>
<p>You could also obtain generalised recall, precision, and F1 scores for all the labels. The way that this works is the recall, precision, and F1 scores are obtained for each class, and then they are averaged out. They can be averaged in three ways:</p>
<ul>
<li><p>Micro-averaged: all samples equally contribute to the final averaged metric (in imbalanced dataset with many more of Class A than Class B, both would be treated differently according to the total true positives, false negatives and false positives).</p></li>
<li><p>Macro-averaged: all classes equally contribute to the final averaged metric (in imbalanced dataset with many more of Class A than Class B, both would be treated equally in calculating the average)</p></li>
<li><p>Weighted-averaged: each classes’ contribution to the average is weighted by support (the number of true instances for each label).</p></li>
</ul>
</section>
</section>
<section id="conclusion-which-metrics-to-use-for-pxtextmining-phase-2" class="level2">
<h2 class="anchored" data-anchor-id="conclusion-which-metrics-to-use-for-pxtextmining-phase-2">Conclusion: which metrics to use for pxtextmining Phase 2?</h2>
<p>Ultimately there is not going to be one magic metric that is going to work for the pxtextmining project. As the dataset is quite imbalanced, and we do want to capture the less well-represented classes, the macro-averaged scores are likely to be helpful. I expect to use the macro-averaged F1 score as a very rough indication in the beginning stages of the project.</p>
<p>However, per-class statistics will likely be most important for performance measurement and especially in the model tuning stage will be helpful for indicating labels that are proving hard to capture.[5] The Hamming and Jaccard scores will also be calculated to give an overall indicator of performance although I anticipate that these will not be the primary focus of model tuning. Unfortunately the metric ‘class balance accuracy’ which was utilised in phase 1 of the pxtextmining project cannot be used for the multilabel models in phase 2.</p>
<p>Do note that there are other metrics available. The ones selected here are the ones that have been most commonly mentioned in the literature, as it makes sense to utilise what has been established as best practice elsewhere for generalisability and interpretability.</p>
</section>
<section id="a-note-on-baselinedummy-models" class="level2">
<h2 class="anchored" data-anchor-id="a-note-on-baselinedummy-models">A note on baseline/dummy models</h2>
<p>When prototyping machine learning models, we need to first establish a baseline of performance - what is the most basic level that we should expect from our model? In the case of an imbalanced dataset, a model which just predicts the mode (the most frequently occurring class) regardless of the input would be a suitable dummy. Other approaches to ‘dummy’ models include a model that just outputs random predictions.</p>
<p>It is useful to know what performance a dummy model would be able to achieve, when considering the performance of any machine learning model. This gives us something for us to compare our outputs to.</p>
</section>
<section id="bibliography" class="level2">
<h2 class="anchored" data-anchor-id="bibliography">Bibliography</h2>
<ol type="1">
<li>L. Mosley, ‘A balanced approach to the multi-class imbalance problem’, Doctor of Philosophy, Iowa State University, Digital Repository, Ames, 2013. doi: 10.31274/etd-180810-3375.</li>
<li>M.-L. Zhang and Z.-H. Zhou, ‘A Review on Multi-Label Learning Algorithms’, IEEE Trans. Knowl. Data Eng., vol.&nbsp;26, no. 8, pp.&nbsp;1819–1837, Aug.&nbsp;2014, doi: 10.1109/TKDE.2013.39.</li>
<li>X.-Z. Wu and Z.-H. Zhou, ‘A Unified View of Multi-Label Performance Measures’, in Proceedings of the 34th International Conference on Machine Learning, Jul.&nbsp;2017, pp.&nbsp;3780–3788. Accessed: Jan.&nbsp;20, 2023. [Online]. Available: https://proceedings.mlr.press/v70/wu17a.html</li>
<li>M. Heydarian, T. E. Doyle, and R. Samavi, ‘MLCM: Multi-Label Confusion Matrix’, IEEE Access, vol.&nbsp;10, pp.&nbsp;19083–19095, 2022, doi: 10.1109/ACCESS.2022.3151048.</li>
<li>S. Henning, W. H. Beluch, A. Fraser, and A. Friedrich, ‘A Survey of Methods for Addressing Class Imbalance in Deep-Learning Based Natural Language Processing’. arXiv, Oct.&nbsp;10, 2022. Accessed: Jan.&nbsp;20, 2023. [Online]. Available: http://arxiv.org/abs/2210.04675</li>
</ol>


</section>

</main> <!-- /main -->
<script type="text/javascript">

    // replace cmd keyboard shortcut w/ control on non-Mac platforms
    const kPlatformMac = typeof navigator !== 'undefined' ? /Mac/.test(navigator.platform) : false;
    if (!kPlatformMac) {
       var kbds = document.querySelectorAll("kbd")
       kbds.forEach(function(kbd) {
          kbd.innerHTML = kbd.innerHTML.replace(/⌘/g, '⌃');
       });
    }
    
    </script>
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">This page is built with <a href="https://quarto.org/">Quarto</a> and is based on the <a href="https://github.com/craig-shenton/quarto-nhs-theme">NHS Theme by Craig Shenton</a></div>   
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item">
    <a class="nav-link" href="https://github.com/CDU-data-science-team/PatientExperience-QDC/blob/main/LICENCE">License</a>
  </li>  
    <li class="nav-item">
    <a class="nav-link" href="https://github.com/CDU-data-science-team/PatientExperience-QDC/blob/main/CODE_OF_CONDUCT.md">Code of Conduct</a>
  </li>  
</ul>
    </div>
  </div>
</footer>



</body></html>